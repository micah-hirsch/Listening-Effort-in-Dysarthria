---
title: "Main Analysis"
author: "Micah E. Hirsch"
date: "2024-05-13"
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Purpose

The purpose of this document is to analyze and report findings related to M. Hirsch's dissertation project. The data preparation and analysis was conducted using R Version 4.3.3.

```{r, echo = F, warning = F, message = F}

# Load in Needed Packages

library(rio)
library(tidyverse)
library(mgcv)
library(itsadug)
library(irr)
library(ggpubr) # installl.packages("ggpubr")
library(gt) # install.packages("gt")
library(gtsummary) # install.packages("gtsummary")
library(glmmTMB) # install.packages("glmmTMB")
library(performance) # install.packages("performance")
library(sjPlot) # install.packages("sjPlot")


```


# Listener Demographics

The table below shows the demographic information for the listeners in this study. A total of 39 listeners were recruited to participate in the study. However, 1 participant withdrew from the study (their data was already destroyed before exporting the listener demographic dataset) and 4 participants were dismissed due to difficulty during the calibration phase of the study. Therefore, there ended up being 34 total listeners in this study. The demographic table shows the demographic information for the 34 participants included in the study (complete) and the 4 who were not able to be tracked by the eye-tracking camera (incomplete).

All listeners reported being a fluent speaker of English. However, two participants noted that their native language was different than American English. Their reported native languages were Turkish and Spanish.

```{r, warning = F, message = F}

demo <- rio::import("Cleaned Data/cleaned_listener_demo.csv") |>
  dplyr::mutate(gender = factor(gender, levels = c("Man", "Woman", "Nonbinary", "Questioning", "Prefer not to answer")),
                ethnicity = factor(ethnicity, levels = c("Hispanic/Latino(a/e)", "Not Hispanic/Latino(a/e)", "Prefer not to answer")),
                race = factor(race, levels = c("white/Caucasian", "Black/African American", "Asian/Asian American",
                                               "Native Hawaiian or Other Pacific Islander", "Native American or Alaska Native",
                                               "Biracial or Multiracial", "Prefer not to answer", "Race not listed")),
                native_lang = factor(native_lang, c("American English", "Not American English")))
  
demo_table <- demo |>
  dplyr::select(age, gender, ethnicity, race, native_lang, pupil_complete) |>
  tbl_strata(
    strata = pupil_complete,
    ~.x |>
      tbl_summary(type = list(age ~ "continuous",
                          gender ~ "categorical",
                          ethnicity ~ "categorical",
                          race ~ "categorical",
                          native_lang ~ "categorical"),
                  statistic = list(all_continuous() ~ "{mean} ({sd}, {min}-{max})",
                               all_categorical() ~ "{n} ({p}%)"),
                  digits = list(everything() ~ c(2)),
                  label = list(age ~ "Age",
                           gender ~ "Gender",
                           ethnicity ~ "Ethnicity",
                           race ~ "Race",
                           native_lang ~ "Native Language"))
    ) |>
  as_gt()

demo_table

demo_table |>
  gt::gtsave("Tables/demographic_table.html")

```


```{r, echo = F, warning = F, message = F}

# Removing unneeded items from the environment
rm(demo, demo_table)

```


# Descriptives

## Transcription Reliability

Phrase repetition accuracy was determined by transcriptions of the listener's spoken response. These reponses were transcribed by 2 research assistants. To determine interrater reliability, each RA transcribed at least 20% of the other RA's listener reponses. Fleiss' kappa was then used to calculated interrater reliability for the resulting phrase repetition accuracy determination (accurate or inaccurate). Based on the result below, our interrater reliability for phrase repetition accuracy was strong.

```{r, warning = F, message = F}

phrase_acc <- rio::import("Cleaned Data/repetition_accuracy.csv")

phrase_acc <- phrase_acc |>
  dplyr::mutate(rep_acc = ifelse(initial_response == "missing data", NA, 
                                 ifelse(target_number == correct_words_initial, "accurate", "inaccurate")),
                rep_acc_rel = ifelse(rel_response == "missing data", NA, 
                                 ifelse(target_number == correct_words_rel, "accurate", "inaccurate")))

reliability <- phrase_acc |>
  dplyr::filter(!is.na(rep_acc_rel)) |>
  dplyr::select(rep_acc, rep_acc_rel)

# Computing Fleiss' Kappa

kappam.fleiss(reliability, detail = T)

phrase_acc_df <- phrase_acc |>
  dplyr::select(subject, trial, target_number, correct_words_initial, rep_acc)

```

## Speaker Characteristics

The speaker characteristics are listed below. Note: 41 trials were removed because they had missing listener responses (i.e. audio file cut out, etc.). Therefore, the summary statistics are based on 2679 trials total.

```{r, warning = F, message = F}

ple_data <- rio::import("Cleaned Data/cleaned_ple_data.csv") |>
  dplyr::left_join(phrase_acc_df, by = c("subject", "trial")) |>
  dplyr::mutate(speaker = factor(speaker, levels = c("Control", "ALS")),
                trial_c = trial - 6,
                rep_acc = factor(rep_acc, levels = c("accurate", "inaccurate")))

speaker_table <- ple_data |>
  dplyr::filter(!is.na(rep_acc)) |>
  dplyr::select(subject, trial, speaker, correct_words_initial, target_number, rep_acc) |>
  dplyr::mutate(intel = (correct_words_initial/target_number)*100) |>
  dplyr::select(-c(subject, trial, correct_words_initial, target_number)) |>
  tbl_summary(by = speaker,
              type = list(intel ~ "continuous",
                          rep_acc ~ "categorical"),
              statistic = list(all_continuous() ~ "{mean} ({sd})",
                               all_categorical() ~ "{n} ({p}%)"),
              digits = list(everything() ~ c(2)),
              label = list(intel ~ "Intelligibility",
                           rep_acc ~ "Repetition Accuracy")) |>
  as_gt()

speaker_table

speaker_table |>
  gt::gtsave("Tables/speaker_table.html")


```


# Perceived Listening Effort Ratings

## Descriptives

The summary statistics for perceived listening effort (PLE) ratings are shown in the table below. There were 5 trials with missing PLE ratings, so those were removed from the analysis. There is a total of 2675 trials represented in the data below.

```{r, warning = F, message = F}

ple_data |>
  dplyr::select(speaker, effort_rating, rep_acc) |>
  # Filtering out trials with cut-off listener responses
  dplyr::filter(!is.na(rep_acc)) |>
  # filtering out trials with missing effort ratings (5 trials missing, 4 additionally removed)
  dplyr::filter(effort_rating > 0) |>
  tbl_strata(
    strata = rep_acc,
    ~.x |>
      tbl_summary(
        by = speaker,
        type = list(effort_rating ~ "continuous"),
        statistic = list(all_continuous() ~ c("{mean} ({sd})")),
        missing = "no",
        digits = all_continuous() ~ 2,
        label = list(effort_rating ~ "Perceived Listening Effort")
      ))

```

## PLE Models

Linear mixed effects (LME) models were used to test the influence of speaker (Control vs ALS) and phrase repetition accuracy (accurate vs inaccurate) on perceived listening effort ratings. We used a model-building approach for this analysis. The steps are detailed below.

### Fully Unconditional Model

```{r, warning = F, message = F}

ple_data <- ple_data |>
  dplyr::filter(!is.na(rep_acc)) |>
  dplyr::filter(effort_rating > 0)

m0_ple <- glmmTMB(effort_rating ~ 1 + (1|subject) + (1|code), data = ple_data)
sjPlot::tab_model(m0_ple, pred.labels = "Intercept",
                  dv.labels = "Perceived Listening Effort")

performance::icc(m0_ple)

```
### PLE Model 1

In this step, we are adding trial order (trial_c) as a random slop for both the listener (subject) and phrase (code) random intercepts. The random intercept of trial order significantly improved model fit.

```{r, warning = F, message = F}

m1_ple <- glmmTMB(effort_rating ~ 1 + (trial_c|subject) + (trial_c|code), data = ple_data)
sjPlot::tab_model(m1_ple, pred.labels = "Intercept",
                  dv.labels = "Perceived Listening Effort")
performance::test_performance(m0_ple, m1_ple)

```
### PLE Model 2

Trial order is the first random effect added to the model. Trial order is added in order to control for any possible order effects in the data. As noted in the results below, adding trial order did not significantly improve model fit to the data. However, I'll leave this fixed effect in the model to act as a covariate.

```{r, warning = F, message = F}

m2_ple <- glmmTMB(effort_rating ~ trial_c + (trial_c|subject) + (trial_c|code), data = ple_data)
sjPlot::tab_model(m2_ple, 
                  pred.labels = c("Intercept", "Trial Order"),
                  dv.labels = "Perceived Listening Effort")

performance::test_performance(m1_ple, m2_ple)

```

### PLE Model 3

Adding the main fixed effect of speaker significantly improved the model fit.

```{r, warning = F, message = F}

m3_ple <- glmmTMB(effort_rating ~ trial_c + speaker + (trial_c|subject) + (trial_c|code), data = ple_data)
sjPlot::tab_model(m3_ple,
                  pred.labels = c("Intercept", "Trial Order", "Speaker [ALS]"),
                  dv.labels = "Perceived Listening Effort")

performance::test_performance(m2_ple, m3_ple)

```

### PLE Model 4

The main fixed effect of repetition accuracy significantly improved model fit.

```{r, warning = F, message = F}

m4_ple <- glmmTMB(effort_rating ~ trial_c + speaker + rep_acc + (trial_c|subject) + (trial_c|code), data = ple_data)
sjPlot::tab_model(m4_ple,
                  pred.labels = c("Intercept", "Trial Order", "Speaker [ALS]", "Repetition Accuracy [Inaccurate]"),
                  dv.labels = "Perceived Listening Effort")

performance::test_performance(m3_ple, m4_ple)


```
### PLE Model 5

The interaction term between speaker and repetition accuracy did not significantly improve model fit. This indicates that the magnitude between accurate and inaccurate ratings was similar for both speakers.

```{r, warning = F, message = F}

m5_ple <- glmmTMB(effort_rating ~ trial_c + speaker + rep_acc + speaker*rep_acc + (trial_c|subject) + (trial_c|code), data = ple_data)
sjPlot::tab_model(m5_ple,
                  pred.labels = c("Intercept", "Trial Order", "Speaker [ALS]", "Repetition Accuracy [Inaccurate]",
                                  "Speaker [ALS] * Repetition Accuracy [Inaccurate]"),
                  dv.labels = "Perceived Listening Effort")

performance::test_performance(m4_ple, m5_ple)


```

### Final PLE Model with Unstandardized Estimates

Therefore, the best-fitting model for the data is model 4 with just the main effects of speaker and repetition accuracy. The results indicate that, on average, the ALS speaker was rated as more effortful by the listeners compared to the control speaker for accurately recognized phrases, controlling for trial order. Inaccurate repetitions, on average, were rated higher than accurately recognized phrases for the control speaker by the listeners. Since the interaction between speaker and repetition accuracy was not significant, this indicates the difference in PLE scores between accurately and inaccurately recognized phrases was similar between the two speakers.

```{r, warning = F, message = F}

final_ple <- glmmTMB(effort_rating ~ trial_c + speaker + rep_acc + (trial_c|subject) + (trial_c|code), data = ple_data)
sjPlot::tab_model(final_ple,
                  pred.labels = c("Intercept", "Trial Order", "Speaker [ALS]", "Repetition Accuracy [Inaccurate]"),
                  dv.labels = "Perceived Listening Effort",
                  file = "Tables/ple_mod_unstd.html")

```


### Final PLE Model with Standardized Estimates

This model shows the standardized estimates for the final PLE model.

```{r, warning = F, message = F}

final_ple_std <- glmmTMB(scale(effort_rating) ~ scale(trial_c) + speaker + rep_acc + 
                    (trial_c|subject) + (trial_c|code), data = ple_data)
sjPlot::tab_model(final_ple_std,
                  pred.labels = c("Intercept", "Trial Order", "Speaker [ALS]", "Repetition Accuracy [Inaccurate]"),
                  dv.labels = "Perceived Listening Effort",
                  file = "Tables/ple_mod_std.html")

```


```{r, eval = F}

sjPlot::tab_model(m1_ple, m2_ple, m3_ple, m4_ple, m5_ple,
                  pred.labels = c("Intercept", "Trial Order", "Speaker [ALS]", "Repetition Accuracy [Inaccurate]",
                                  "Speaker [ALS] * Repetition Accuracy [Inaccurate]"),
                  dv.labels = c("Model 1", "Model 2", "Model 3", "Model 4", "Model 5"),
                  file = "Tables/ple_models.html")

```


## Plot

```{r, warning = F, message = F}

my_pal <- c("#4E4187", "#3083DC", "#F8FFE5", "#7DDE92", "#2EBFA5")

ple_data |>
  dplyr::group_by(speaker, rep_acc) |>
  dplyr::summarize(per_effort = mean(effort_rating, na.rm = T), 
                   sd = sd(effort_rating, na.rm = T),
                   se = sd/sqrt(n())) |>
  ggplot() +
   aes(x = rep_acc,
       y = per_effort,
       group = speaker,
       color = speaker,
       fill = speaker) +
  geom_bar(stat = "identity", alpha = 0.6, position = position_dodge()) +
  geom_errorbar(aes(ymin = per_effort - se, ymax = per_effort + se), width = 0.4, position = position_dodge(.9)) +
  labs(x = "Accuracy", y = "Perceived Listening Effort Rating") +
  theme_bw() +
  theme(aspect.ratio = 1) +
  scale_color_manual(values = c(my_pal[1], my_pal[5])) +
  scale_fill_manual(values = c(my_pal[1], my_pal[5]))

ggsave("Figures/ple_plot.png", plot = last_plot())

```

```{r, echo = F}

rm(final_ple, final_ple_std, m0_ple, m1_ple, m2_ple, m3_ple, m4_ple, m5_ple, reliability, speaker_table)

```


# Pupil Dilation

## Total Number of Trials

A total of 2654 trials are included in the analysis

```{r}

pupil_df <- rio::import("Cleaned Data/cleaned_pupil_data_normalized.csv") |>
  # Merging phrase accuracy information with df
  dplyr::left_join(phrase_acc_df, by = c("subject", "trial")) |>
  # Filter out trials with missing phrase repetition data |>
  dplyr::filter(!is.na(rep_acc))

pupil_df |>
  dplyr::select(subject, trial) |>
  dplyr::distinct() |>
  dplyr::summarize(n = n())

```


## Pupil Dilation Models

### Initial Model

Initial model with smooths for speaker and phrase.

```{r}

# Narrowing to the analysis region of interest

pupil_roi <- pupil_df |>
  # filtering out timebins below 500 (i.e., keeping timebins above 500 ms)
  dplyr::filter(time_norm >= 500) |>
  dplyr::group_by(subject, trial) |>
  # filtering out timebins 1000 ms after phrase offset
  dplyr::filter(time_norm<= max(time_norm) - 2000) |>
  ungroup() |>
  dplyr::mutate(speaker = factor(speaker, levels = c("Control", "ALS")),
                rep_acc = factor(rep_acc, levels = c("accurate", "inaccurate")),
                phrase = as.factor(code),
                listener = as.factor(subject),
                condition = case_when(speaker == "Control" & rep_acc == "accurate" ~ "Control Accurate",
                                      speaker == "Control" & rep_acc == "inaccurate" ~ "Control Inaccurate",
                                      speaker == "ALS" & rep_acc == "accurate" ~ "ALS Accurate",
                                      TRUE ~ "ALS Inaccurate"),
                condition = factor(condition, 
                                   levels = c("Control Accurate", "Control Inaccurate", "ALS Accurate", "ALS Inaccurate"))) 

m0_pupil <- bam(normed_pupil ~ condition +
                  s(time_norm, by = condition, k = 20) +
                  # Random Smooths for listener and phrase
                  s(time_norm, listener, bs = 'fs', m =1) +
                  s(time_norm, phrase, bs = 'fs', m = 1),
                discrete = T,
                data = pupil_roi)

summary(m0_pupil)

plot_smooth(m0_pupil,
            view = "time_norm",
            plot_all = "condition",
            rug = F, se = 1, rm.ranef = T)

```


#### Model Check

```{r}

gam.check(m0_pupil)

acf_resid(m0_pupil)

```

### Model 1

Adding in AR(1) structure to address autocorrelation concerns. Also increased knots based on output from the initial model, and took away the random smooth for phrase due to model convergence issues.

```{r}

# Marking start of trials

pupil_roi <- pupil_roi |>
  dplyr::group_by(listener, speaker) |>
  dplyr::mutate(num_points = n()) |>
  dplyr::mutate(start_event = c(TRUE, rep(FALSE, each = (num_points - 1)))) |>
  ungroup()
  
m1_pupil <- bam(normed_pupil ~ condition +
                  s(time_norm, by = condition, k = 25) +
                  # Random Smooths for listener and phrase
                  s(time_norm, listener, bs = 'fs', m =1),
                discrete = T,
                AR.start = start_event,
                rho = start_value_rho(m0_pupil),
                data = pupil_roi)

summary(m1_pupil)

plot_smooth(m1_pupil,
            view = "time_norm",
            plot_all = "condition",
            rug = F, se = 1, rm.ranef = T)

```

#### Model Check

Autocorrelation improved. This model's residuals also look better, but there are still some heavy residual tails.

```{r}

gam.check(m1_pupil)

acf_resid(m1_pupil)

```

### Model 2 

Adding scaled t distribution to fix model fit.

```{r}

m2_pupil <- bam(normed_pupil ~ condition +
                  s(time_norm, by = condition, k = 25) +
                  # Random Smooths for listener and phrase
                  s(time_norm, listener, bs = 'fs', m =1),
                discrete = T,
                AR.start = start_event,
                rho = start_value_rho(m0_pupil),
                family = "scat",
                data = pupil_roi)

summary(m2_pupil)

m2_prediction <- plot_smooth(m2_pupil,
            view = "time_norm",
            plot_all = "condition",
            rug = F, se = 1, rm.ranef = T)

```

#### Model Check

Model residuals look better, but not great

```{r}

gam.check(m2_pupil)

acf_resid(m2_pupil)

```

#### Difference Plots

```{r}

plot_diff(m2_pupil, view = "time_norm",
          comp = list(condition = c("ALS Accurate", "Control Accurate")),
          rm.ranef = T)

plot_diff(m2_pupil, view = "time_norm",
          comp = list(condition = c("Control Inaccurate", "Control Accurate")),
          rm.ranef = T)

plot_diff(m2_pupil, view = "time_norm",
          comp = list(condition = c("ALS Inaccurate", "ALS Accurate")),
          rm.ranef = T)

```
## Plot

Draft pupil dilation plot

I am trying to find a way to predict model-predicted values against the actual data. geom_smooth doesn't consider the random effects, so I am thinking I need to extract model data from the final model to show the goodness of fit

It might be good to do this for every iteration of the model to show the process too...

```{r}

ind_pupil_df <- pupil_df |>
  dplyr::group_by(speaker, subject, rep_acc, time_norm) |>
  summarize(pupil = mean(normed_pupil, na.rm = T),
            se = sd(normed_pupil, na.rm = T)/sqrt(n()))

sum_pupil_df <- ind_pupil_df |>
  dplyr::group_by(speaker, rep_acc, time_norm) |>
  summarize(m_pupil = mean(pupil, na.rm = T),
            se_pupil = sd(pupil, na.rm = T)/sqrt(n()))

# I'm trying to find a way to plot predicted values against actual data.
predicted_values <- m2_prediction[[1]][c("condition", "time_norm", "ul", "ll")]

sum_pupil_df |>
  ggplot() +
  aes(x = time_norm,
      y = m_pupil,
      color = speaker,
      fill = speaker) +
  geom_smooth() +
  geom_line(size = 1) +
  geom_vline(xintercept = 0, size = 0.35) +
  geom_vline(xintercept = 1910, linetype = 2) +
  annotate("rect", xmin = 500, xmax = 2910, ymin = 0, ymax = 200, alpha = 0.2) +
  coord_cartesian(ylim = c(-10, 250), xlim = c(0, 4910)) +
  facet_wrap("rep_acc") +
  theme_bw() +
  theme(aspect.ratio = 1,
        legend.position = "bottom") +
  labs(x = "Normalized Time (ms)",
       y = "Pupil Dilation (Arbitrary Units)") +
  scale_color_manual(values = c(my_pal[1], my_pal[5])) +
  scale_fill_manual(values = c(my_pal[1], my_pal[5]))

ggsave("Figures/pupil_plot.png", plot = last_plot())

```

